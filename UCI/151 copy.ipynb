{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI Dataset: Log-Linear Model v.s. Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import ld\n",
    "from utlis import vectorize_tensor, reconstruct_tensor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    cp.random.seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_random_seed(42)\n",
    "k = 5\n",
    "bandwidth = 0.1\n",
    "bandwidth_AE = 0.1\n",
    "eps = np.asarray(1.0e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCI_id = 59 #\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15\n",
      "Shape of features: (20000, 16)\n",
      "Shape of labels: (20000,)\n",
      "Unique labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25]\n",
      "Shape of features: (16000, 16)\n"
     ]
    }
   ],
   "source": [
    "UCI_dataset = fetch_ucirepo(id=UCI_id)\n",
    "\n",
    "X = np.array(UCI_dataset.data.features)\n",
    "Y = np.array(LabelEncoder().fit_transform(UCI_dataset.data.targets))\n",
    "\n",
    "# Check whether features are all positive\n",
    "print(X.min(), X.max())\n",
    "\n",
    "if X.min() < 0:\n",
    "    raise ValueError(\"Features are not all positive\")\n",
    "\n",
    "\n",
    "# Print the shape of features and labels\n",
    "print(\"Shape of features:\", X.shape)\n",
    "print(\"Shape of labels:\", Y.shape)\n",
    "\n",
    "# Find unique labels\n",
    "unique_labels = np.unique(Y)\n",
    "print(\"Unique labels:\", unique_labels)\n",
    "\n",
    "# Create an array of indices\n",
    "indices = np.arange(len(Y))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Use the shuffled indices to randomly select data for training and testing\n",
    "n_train = int(len(Y) * (1 - test_size))\n",
    "X_train, Y_train = X[indices[:n_train]], Y[indices[:n_train]]\n",
    "X_test, Y_test = X[indices[n_train:]], Y[indices[n_train:]]\n",
    "\n",
    "print(\"Shape of features:\", X_train.shape)\n",
    "\n",
    "X_train_class = []\n",
    "Y_train_class = []\n",
    "for i in unique_labels:\n",
    "    X_train_class.append(X_train[np.isin(Y_train, i).flatten()])\n",
    "    Y_train_class.append(Y_train[np.isin(Y_train, i).flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Structure of the Feature: (2, 2, 2, 2)\n",
      "Number of new samples per class: 98\n"
     ]
    }
   ],
   "source": [
    "# Feature dimension\n",
    "D = X_train.shape[1]\n",
    "S = (2, 2, 2, 2)\n",
    "print(\"Tensor Structure of the Feature:\", S)\n",
    "\n",
    "if np.prod(S) != D:\n",
    "    raise ValueError(\"The product of the tensor structure is not equal to the feature dimension\")\n",
    "\n",
    "# 20% of the data of the training set\n",
    "num_new_samples = int((1 - test_size) * len(Y_train) * 0.2 // len(unique_labels))\n",
    "print(\"Number of new samples per class:\", num_new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (numpy array): NumPy array of shape (num, 28*28)\n",
    "            labels (numpy array): Corresponding labels for each image\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the vector and label for a given index\n",
    "        vector = self.data[idx].astype(np.float32)  # No reshaping, as data is general vector signal\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            vector = self.transform(vector)\n",
    "\n",
    "        return vector, label\n",
    "\n",
    "train_data_original = np.array(X_train)\n",
    "labels = Y_train\n",
    "custom_train_dataset = CustomDataset(train_data_original, labels)\n",
    "train_loader_original = DataLoader(dataset=custom_train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation with Log-LInear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legendre Decomposition (Many-Body Approximation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 16 into shape (2,3,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/pbb/miniconda3/envs/IG/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"/home/pbb/miniconda3/envs/IG/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pbb/miniconda3/envs/IG/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/pbb/miniconda3/envs/IG/lib/python3.11/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_2571134/869057452.py\", line 9, in LD_helper\nValueError: cannot reshape array of size 16 into shape (2,3,5)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     _, _, scaleX, _, theta \u001b[38;5;241m=\u001b[39m ld\u001b[38;5;241m.\u001b[39mLD(X_train_class[class_][i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mS), B\u001b[38;5;241m=\u001b[39mB_LD, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-1\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (scaleX, theta)\n\u001b[0;32m---> 12\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLD_helper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munique_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_train_class\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_ \u001b[38;5;129;01min\u001b[39;00m unique_labels:\n\u001b[1;32m     15\u001b[0m     scaleX_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/IG/lib/python3.11/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IG/lib/python3.11/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IG/lib/python3.11/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/IG/lib/python3.11/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/IG/lib/python3.11/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/IG/lib/python3.11/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 16 into shape (2,3,5)"
     ]
    }
   ],
   "source": [
    "B_LD = ld.default_B(S, 2, cp.get_array_module(X_train[0]))\n",
    "\n",
    "print(B_LD.shape)\n",
    "\n",
    "scaleX_class = []\n",
    "theta_class = []\n",
    "\n",
    "def LD_helper(i, class_):\n",
    "    _, _, scaleX, _, theta = ld.LD(X_train_class[class_][i].reshape(*S), B=B_LD, verbose=False, n_iter=1000, lr=1e-1)\n",
    "    return (scaleX, theta)\n",
    "\n",
    "results = Parallel(n_jobs=30)(delayed(LD_helper)(i, class_) for class_ in unique_labels for i in range(len(X_train_class[class_])))\n",
    "\n",
    "for class_ in unique_labels:\n",
    "    scaleX_list = []\n",
    "    theta_list = []\n",
    "\n",
    "    for i in range(len(X_train_class[class_])):\n",
    "        result = results[i + class_ * len(X_train_class[class_])]\n",
    "\n",
    "        scaleX_list.append(result[0])\n",
    "        theta_list.append(result[1])\n",
    "\n",
    "    scaleX_class.append(scaleX_list)\n",
    "    theta_class.append(theta_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting and Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_theta_class = []\n",
    "\n",
    "for class_ in unique_labels:\n",
    "    reduced_theta = vectorize_tensor(np.array(theta_class[class_]), B_LD)\n",
    "\n",
    "    # Fit a KDE to the theta values\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(reduced_theta)\n",
    "    # Sample new data from the KDE\n",
    "    sampled_reduced_theta = kde.sample(n_samples=num_new_samples)\n",
    "\n",
    "    sampled_theta = reconstruct_tensor(sampled_reduced_theta, (num_new_samples, *S), B_LD)\n",
    "\n",
    "    sampled_theta_class.append(sampled_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Submanifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 4)\n"
     ]
    }
   ],
   "source": [
    "# Construct the constrained coordinates\n",
    "B_BP = ld.default_B(S, 1, cp.get_array_module(X_train[0]))\n",
    "# B_BP = B_LD\n",
    "\n",
    "print(B_BP.shape)\n",
    "\n",
    "# Compute every datapoint's eta_hat (served as the linear constraints)\n",
    "eta_hat_class = []\n",
    "\n",
    "for class_ in unique_labels:\n",
    "    eta_hat_list = []\n",
    "    for i in range(X_train_class[class_].shape[0]):\n",
    "        xp = cp.get_array_module(X_train_class[class_][i])\n",
    "        P = (X_train_class[class_][i].reshape(*S) + eps) / scaleX_class[class_][i]\n",
    "        eta_hat = ld.get_eta(P, len(S), xp)\n",
    "        eta_hat_list.append(eta_hat)\n",
    "    eta_hat_list = cp.asarray(eta_hat_list)\n",
    "\n",
    "    eta_hat_class.append(eta_hat_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not Converged. Consider increasing the number of iterations.\n",
      "Warning: Not Converged. Consider increasing the number of iterations.\n",
      "Warning: Not Converged. Consider increasing the number of iterations.\n",
      "Warning: Not Converged. Consider increasing the number of iterations.\n",
      "Warning: Not Converged. Consider increasing the number of iterations.\n",
      "Warning: Not Converged. Consider increasing the number of iterations.\n"
     ]
    }
   ],
   "source": [
    "def BP_helper(i, class_):\n",
    "    N = ld.kNN(sampled_theta_class[class_][i], np.array(theta_class[class_]), k=k)\n",
    "    avg_scale = np.mean(np.array(scaleX_class[class_])[N])\n",
    "    avg_eta_hat = np.mean(eta_hat_class[class_][N], axis=0)\n",
    "\n",
    "    _, _, P, theta = ld.BP(sampled_theta_class[class_][i], [(X_train_class[class_][j].reshape(*S) + eps) / scaleX_class[class_][j] for j in N], avg_eta_hat, avg_scale, B=B_BP, verbose=False, n_iter=1000, lr=5e-2, exit_abs=True)\n",
    "    X_recons_ = P.reshape(-1)\n",
    "    return (theta, X_recons_)\n",
    "\n",
    "results = Parallel(n_jobs=30)(delayed(BP_helper)(i, class_) for i in range(num_new_samples) for class_ in unique_labels)\n",
    "\n",
    "sampled_theta_BP_class = []\n",
    "X_recons_class = []\n",
    "\n",
    "for class_ in unique_labels:\n",
    "    sampled_theta_BP = []\n",
    "    sampled_X_recons = []\n",
    "    X_recons_list = []\n",
    "    for i in range(num_new_samples):\n",
    "        result = results[i + num_new_samples * class_]\n",
    "\n",
    "        sampled_theta_BP.append(result[0])\n",
    "        sampled_X_recons.append(result[1])\n",
    "\n",
    "    sampled_theta_BP_class.append(np.array(sampled_theta_BP))\n",
    "    X_recons_class.append(np.array(sampled_X_recons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_LD = []\n",
    "\n",
    "for class_ in unique_labels:\n",
    "    for i in range(num_new_samples):\n",
    "        augmented_data_LD.append(X_recons_class[class_][i])\n",
    "\n",
    "train_data_LD = np.array(augmented_data_LD)\n",
    "labels = np.repeat(unique_labels, num_new_samples)\n",
    "custom_train_dataset = CustomDataset(train_data_LD, labels)\n",
    "train_loader_LD = DataLoader(dataset=custom_train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation with Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size=D, hidden_size=D//4, z_dim=B_LD.shape[0]):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        # self.fc2 = nn.Linear(hidden_size , hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size, z_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self , x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size=D, hidden_size=D//4, z_dim=B_LD.shape[0]):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(z_dim , hidden_size)\n",
    "        # self.fc2 = nn.Linear(hidden_size , hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self , x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder().to(device)\n",
    "dec = Decoder().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer_enc = torch.optim.Adam(enc.parameters())\n",
    "optimizer_dec = torch.optim.Adam(dec.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:08<00:00,  1.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9b54232010>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = []\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_epoch_loss = 0\n",
    "    for (x , _) in train_loader_original:\n",
    "        x = x.to(device)\n",
    "        #100 , 1 , 28 , 28 ---> (100 , 28*28)\n",
    "        x = x.flatten(1)\n",
    "        latents = enc(x)\n",
    "        output = dec(latents)\n",
    "        loss = loss_fn(output , x)\n",
    "        train_epoch_loss += loss.cpu().detach().numpy()\n",
    "        optimizer_enc.zero_grad()\n",
    "        optimizer_dec.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_enc.step()\n",
    "        optimizer_dec.step()\n",
    "    train_loss.append(train_epoch_loss)\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = None\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (xs , labels) in train_loader_original:\n",
    "        xs = xs.to(device)\n",
    "        xs = xs.flatten(1)\n",
    "        all_labels.extend(list(labels.numpy()))\n",
    "        latents = enc(xs)\n",
    "        if representation is None:\n",
    "            representation = latents.cpu()\n",
    "        else:\n",
    "            representation = torch.vstack([representation , latents.cpu()])\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "representation = representation.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_AE_class = []\n",
    "\n",
    "for class_ in unique_labels:\n",
    "    sampled_AE_list = []\n",
    "\n",
    "    rep = representation[np.argwhere(all_labels == class_)].squeeze()\n",
    "    # Fit a KDE to the theta values\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth_AE).fit(rep)\n",
    "\n",
    "    # Sample new data from the KDE\n",
    "    sampled_rep = kde.sample(n_samples=num_new_samples)\n",
    "    for i in range(num_new_samples):\n",
    "        pred = dec(torch.Tensor(sampled_rep[i])[None , ...].to(device)).cpu().detach().numpy()\n",
    "        sampled_AE_list.append(pred)\n",
    "\n",
    "    sampled_AE_class.append(sampled_AE_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_AE = []\n",
    "\n",
    "for class_ in unique_labels:\n",
    "    for i in range(num_new_samples):\n",
    "        augmented_data_AE.append(sampled_AE_class[class_][i].flatten())\n",
    "\n",
    "train_data_AE = np.array(augmented_data_AE)\n",
    "labels = np.repeat(unique_labels, num_new_samples)\n",
    "\n",
    "custom_train_dataset = CustomDataset(train_data_AE, labels)\n",
    "train_loader_AE = DataLoader(dataset=custom_train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=10):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)  # Linear layer for general input size\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)   # Apply linear transformation\n",
    "        return out\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=5, device='cpu'):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for vectors, labels in train_loader:\n",
    "            vectors, labels = vectors.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(vectors)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Function to test the model\n",
    "def test_model(model, test_loader, device='cpu'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for vectors, labels in test_loader:\n",
    "            vectors, labels = vectors.to(device), labels.to(device)\n",
    "            outputs = model(vectors)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Main function to run training and testing on a dataset\n",
    "def bootstrapping(train_loader, test_dataset, input_size, num_classes=10, num_epochs=5, learning_rate=0.05, device='cpu'):\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = LogisticRegressionModel(input_size=input_size, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs=num_epochs, device=device)\n",
    "\n",
    "    # Number of bootstrap resamples\n",
    "    n_bootstrap = 20\n",
    "    accuracies = []\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for i in range(n_bootstrap):\n",
    "        # Randomly sample 500 examples from the dataset with replacement\n",
    "        indices = torch.randint(len(test_dataset), size=(50,))\n",
    "        bootstrap_subset = Subset(test_dataset, indices)\n",
    "        bootstrap_loader = DataLoader(dataset=bootstrap_subset, batch_size=50, shuffle=False)\n",
    "\n",
    "        # Calculate accuracy on the bootstrap sample\n",
    "        accuracy = test_model(model, bootstrap_loader, device=device)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Compute the mean and standard deviation of accuracy\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "\n",
    "    # Calculate 95% confidence interval (mean ± 1.96 * standard deviation)\n",
    "    confidence_interval = (mean_accuracy - 1.96 * std_accuracy, mean_accuracy + 1.96 * std_accuracy)\n",
    "\n",
    "    print(f\"Mean accuracy: {mean_accuracy:.2f}%\")\n",
    "    print(f\"95% confidence interval: ({confidence_interval[0]:.2f}%, {confidence_interval[1]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6759\n",
      "Epoch [2/20], Loss: 0.6686\n",
      "Epoch [3/20], Loss: 0.6592\n",
      "Epoch [4/20], Loss: 0.6509\n",
      "Epoch [5/20], Loss: 0.6434\n",
      "Epoch [6/20], Loss: 0.6321\n",
      "Epoch [7/20], Loss: 0.6320\n",
      "Epoch [8/20], Loss: 0.6243\n",
      "Epoch [9/20], Loss: 0.6262\n",
      "Epoch [10/20], Loss: 0.6175\n",
      "Epoch [11/20], Loss: 0.6085\n",
      "Epoch [12/20], Loss: 0.5931\n",
      "Epoch [13/20], Loss: 0.5953\n",
      "Epoch [14/20], Loss: 0.5809\n",
      "Epoch [15/20], Loss: 0.5854\n",
      "Epoch [16/20], Loss: 0.5857\n",
      "Epoch [17/20], Loss: 0.5674\n",
      "Epoch [18/20], Loss: 0.5714\n",
      "Epoch [19/20], Loss: 0.5621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 0.5660\n",
      "Mean accuracy: 60.10%\n",
      "95% confidence interval: (48.64%, 71.56%)\n"
     ]
    }
   ],
   "source": [
    "bootstrapping(train_loader_original, test_dataset, input_size=X_train.shape[1], num_classes=len(unique_labels), num_epochs=20, learning_rate=0.05, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.7494\n",
      "Epoch [2/20], Loss: 0.7266\n",
      "Epoch [3/20], Loss: 0.7268\n",
      "Epoch [4/20], Loss: 0.6960\n",
      "Epoch [5/20], Loss: 0.7177\n",
      "Epoch [6/20], Loss: 0.6936\n",
      "Epoch [7/20], Loss: 0.6825\n",
      "Epoch [8/20], Loss: 0.6707\n",
      "Epoch [9/20], Loss: 0.6684\n",
      "Epoch [10/20], Loss: 0.6709\n",
      "Epoch [11/20], Loss: 0.6781\n",
      "Epoch [12/20], Loss: 0.6583\n",
      "Epoch [13/20], Loss: 0.6617\n",
      "Epoch [14/20], Loss: 0.6702\n",
      "Epoch [15/20], Loss: 0.6722\n",
      "Epoch [16/20], Loss: 0.6615\n",
      "Epoch [17/20], Loss: 0.6517\n",
      "Epoch [18/20], Loss: 0.6602\n",
      "Epoch [19/20], Loss: 0.6640\n",
      "Epoch [20/20], Loss: 0.6454\n",
      "Mean accuracy: 54.20%\n",
      "95% confidence interval: (39.38%, 69.02%)\n"
     ]
    }
   ],
   "source": [
    "bootstrapping(train_loader_LD, test_dataset, input_size=X_train.shape[1], num_classes=len(unique_labels), num_epochs=20, learning_rate=0.05, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6970\n",
      "Epoch [2/20], Loss: 0.6789\n",
      "Epoch [3/20], Loss: 0.6831\n",
      "Epoch [4/20], Loss: 0.6809\n",
      "Epoch [5/20], Loss: 0.6740\n",
      "Epoch [6/20], Loss: 0.6733\n",
      "Epoch [7/20], Loss: 0.6672\n",
      "Epoch [8/20], Loss: 0.6630\n",
      "Epoch [9/20], Loss: 0.6698\n",
      "Epoch [10/20], Loss: 0.6705\n",
      "Epoch [11/20], Loss: 0.6678\n",
      "Epoch [12/20], Loss: 0.6652\n",
      "Epoch [13/20], Loss: 0.6626\n",
      "Epoch [14/20], Loss: 0.6551\n",
      "Epoch [15/20], Loss: 0.6822\n",
      "Epoch [16/20], Loss: 0.6744\n",
      "Epoch [17/20], Loss: 0.6714\n",
      "Epoch [18/20], Loss: 0.6594\n",
      "Epoch [19/20], Loss: 0.6455\n",
      "Epoch [20/20], Loss: 0.6650\n",
      "Mean accuracy: 66.30%\n",
      "95% confidence interval: (55.54%, 77.06%)\n"
     ]
    }
   ],
   "source": [
    "bootstrapping(train_loader_AE, test_dataset, input_size=X_train.shape[1], num_classes=len(unique_labels), num_epochs=20, learning_rate=0.05, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original and Augmented Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log-Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6889\n",
      "Epoch [2/20], Loss: 0.6765\n",
      "Epoch [3/20], Loss: 0.6657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 0.6567\n",
      "Epoch [5/20], Loss: 0.6519\n",
      "Epoch [6/20], Loss: 0.6427\n",
      "Epoch [7/20], Loss: 0.6367\n",
      "Epoch [8/20], Loss: 0.6308\n",
      "Epoch [9/20], Loss: 0.6215\n",
      "Epoch [10/20], Loss: 0.6239\n",
      "Epoch [11/20], Loss: 0.6157\n",
      "Epoch [12/20], Loss: 0.6134\n",
      "Epoch [13/20], Loss: 0.6059\n",
      "Epoch [14/20], Loss: 0.6015\n",
      "Epoch [15/20], Loss: 0.5962\n",
      "Epoch [16/20], Loss: 0.5918\n",
      "Epoch [17/20], Loss: 0.5898\n",
      "Epoch [18/20], Loss: 0.5849\n",
      "Epoch [19/20], Loss: 0.5821\n",
      "Epoch [20/20], Loss: 0.5780\n",
      "Mean accuracy: 77.40%\n",
      "95% confidence interval: (64.63%, 90.17%)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.vstack([train_data_original, train_data_LD])\n",
    "labels = np.hstack([Y_train, np.repeat(unique_labels, num_new_samples)])\n",
    "custom_train_dataset = CustomDataset(train_data, labels)\n",
    "\n",
    "train_loader = DataLoader(dataset=custom_train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "bootstrapping(train_loader, test_dataset, input_size=X_train.shape[1], num_classes=len(unique_labels), num_epochs=20, learning_rate=0.05, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.7129\n",
      "Epoch [2/20], Loss: 0.6959\n",
      "Epoch [3/20], Loss: 0.6933\n",
      "Epoch [4/20], Loss: 0.6717\n",
      "Epoch [5/20], Loss: 0.6664\n",
      "Epoch [6/20], Loss: 0.6572\n",
      "Epoch [7/20], Loss: 0.6507\n",
      "Epoch [8/20], Loss: 0.6472\n",
      "Epoch [9/20], Loss: 0.6312\n",
      "Epoch [10/20], Loss: 0.6285\n",
      "Epoch [11/20], Loss: 0.6225\n",
      "Epoch [12/20], Loss: 0.6162\n",
      "Epoch [13/20], Loss: 0.6097\n",
      "Epoch [14/20], Loss: 0.6041\n",
      "Epoch [15/20], Loss: 0.6010\n",
      "Epoch [16/20], Loss: 0.5976\n",
      "Epoch [17/20], Loss: 0.5905\n",
      "Epoch [18/20], Loss: 0.5888\n",
      "Epoch [19/20], Loss: 0.5817\n",
      "Epoch [20/20], Loss: 0.5804\n",
      "Mean accuracy: 64.80%\n",
      "95% confidence interval: (51.95%, 77.65%)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.vstack([train_data_original, train_data_AE])\n",
    "labels = np.hstack([Y_train, np.repeat(unique_labels, num_new_samples)])\n",
    "custom_train_dataset = CustomDataset(train_data, labels)\n",
    "\n",
    "train_loader = DataLoader(dataset=custom_train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "bootstrapping(train_loader, test_dataset, input_size=X_train.shape[1], num_classes=len(unique_labels), num_epochs=20, learning_rate=0.05, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
